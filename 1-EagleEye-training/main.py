"""Main script to perform the transformer training.

Fit an encoder-transformer on sequences, for binary classification.
At this point, the sequence dataset is a generated by a random number generator.
You should use a real process behavior dataset instead. The code to generate a real dataset will soon be available
  under '3-Feature-extraction'.
"""

import numpy as np
import random
from typing import Tuple

from train_transformer import TransformerTraining

# TODO: set these hyper parameters to reasonable values, or perform a hyper parameter search

# each behavior event is represented by a vector of this dimension
DIM_SECURITY_FEATURE_VECTOR = 190
# number of training epochs
EPOCHS = 3
# number of epochs to wait for a training improvement, before an early stop
PATIENCE = 3
# the dimesion of tokens within the transformer
D_MODEL = 60
# number of attention heads for multi-head self attention
ATTENTION_HEADS = 8
# the transformer encoder stack size
NUM_ENCODERS = 4
# training batch size
BATCH_SIZE = 64
# length of behavior sequences
WINDOW_SIZE = 200
# the period after which the positional embedding gets repeated
POSITIONAL_ENCODING_PERIOD = 5 * WINDOW_SIZE
# the training learning rate
LR = 0.00001
# the dropout rate throughout the transformer
DROPOUT = 0.1
# the regularization in the final dense layers at the top of the encoder stack
REGULARIZATION = 0.01
# the dimension of keys for the attention mechanism
KEY_DIM = D_MODEL
# the internal dimension in the feed-forward neural network, within each encoder
DIM_FF = 4 * D_MODEL

def create_dummy_dataset_random(dim_security_features: int, window_size: int, num_train_sequences: int, num_val_sequences: int) -> Tuple[np.ndarray]:
    """Create a random sequence dataset.
    
    The resulting data samples will have dimension (number of samples, window_size, dim_security_features).
    The resulting labels will have dimension (number of samples, 2), where each individual label is one-hot encoded.

    :param dim_security_features: Each behavior event is represented by a vector of this dimension.
    :param window_size: The length of each behavior sequence.
    :param num_train_sequences: Number of training samplee.
    :param num_val_sequences: Number of validation samples.
    :return: 4 numpy arrays, which correspond to: training samples, training labels, validation samples, validation labels.
    """
    print("Generating random dataset ...")
    num_samples = num_train_sequences + num_val_sequences
    x = np.random.rand(num_samples, window_size, dim_security_features)
    y = round(0.5 * num_samples) * [0] + round(0.5 * num_samples) * [1]
    random.shuffle(y)
    y = np.array([[t, 1-t] for t in y])
    x_train = x[:num_train_sequences]
    x_val = x[num_train_sequences:]
    y_train = y[:num_train_sequences]
    y_val = y[num_train_sequences:]
    return x_train, y_train, x_val, y_val


# TODO: replace this random dataset with a real, preprocessed process behavior dataset.
x_train, y_train, x_val, y_val = create_dummy_dataset_random(
        dim_security_features=DIM_SECURITY_FEATURE_VECTOR,
        window_size=WINDOW_SIZE,
        num_train_sequences=500,
        num_val_sequences=200)

# perform the transformer training on the training data
transformer_training = TransformerTraining()
transformer_training.train(
        x_train = x_train,
        y_train = y_train,
        x_val = x_val,
        y_val = y_val,
        num_layers = NUM_ENCODERS,
        d_model = D_MODEL,
        num_heads = ATTENTION_HEADS,
        key_dim = KEY_DIM,
        dff = DIM_FF,
        dropout_rate = DROPOUT,
        positional_encoding_period = POSITIONAL_ENCODING_PERIOD,
        regularization = REGULARIZATION,
        learning_rate = LR,
        patience = PATIENCE,
        checkpoint_folder = ".",
        epochs = EPOCHS,
        batch_size = BATCH_SIZE)
# Evaluate the trained model on the validation data.
# The results don't look great? That's to be expected when you train on random data!
transformer_training.perform_evaluation(x_val, y_val)
